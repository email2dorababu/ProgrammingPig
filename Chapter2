Pig does not need to be installed in hadoop cluster.
It runs on the machine from which you launch jadoop jobs.i.e pig is installed on one or more machines that have access to the 
hadoop cluster but are not part of the cluster.(i.e not DN or TT nodes). 
THis machines are called gateway machines or edge machines.
COre of pig is writen in Java, hence it is portable across OSes.
The Shell script that starts Pig is a bash Script, so it requires Unix env.
Set JAVA_HOME.
---------------
PG-12
Cloudera - produces RPMs for RedHat based systems and Packages for use with APT on debian systems.
Also provides TAR bals for other systems that cannot use one of these packages(RPM or APT).
Benefit is you get professional support. But there will be a delay between apache release of PIG and cloudera.

you can download pig from apache's Maven repository. this includes JAR files for Pig,source code and Javadocs as wellas POM file 
that defines pig's dependencies.
Download source from Apache's subversion repository. Also GIT is available.
-------------------------------
RUNNING PIG:
you can run pig locally on your local machine or on your grid. You can also run pig as part of Amazon EMR service.

Running Pig Locally on your machine:
----------------------------------------------
RUnning pig locally is called 'local mode'. local mode is useful for prototyping and debugging your pig latin scripts.
to not to waste cluster resources on small files and small jobs.

0.6 and earlier - pig executed scripts in local mode.
0.7 later uses the Hadoop class 'LocalJobRunner' that reads from local file system and executes MR jobs locally.
This has nice property that job running the same way locally as will on your cluster and all run in one process, making debugging easier.
but it is slow.

Download code from https://github.com/alanfgates/programmingpig
------------------------------------
Running Pig on your Hadoop Cluster:
All of the parsing, checking and planning is done locally(gateway machine). Pig then executes MR jobs in cluster.

The only thing pig needs to know to run on your cluster is - the location of your clusters Namenode and Job tracker.
NN is the manager of HDFS and JT coordinates MR jobs.

These locations can be found in hadoop-site.xml(Hadoop 0.18 and earlier)
or they are in 3 separate files(in Hadoop 0.20 and later) core-site.xml,hdfs-site.xml and Mapred-site.xml.

if these files are present in all nodes it is fine.. if they are not present then copy the files from nodes to a location on 
your gateway machine. this guarantees that you get the proper addresses plus any site-specific settings.
Once you have located,copied or created files, you will need to tell pig the directory they are in by setting PIG_CASSPATH, env
variable to that DIRECTORY, not indivudual files.\

Amazon EMR offer - rather than renting EC2 and other cloud services, EMR allows users to rent virtual Hadoop clusters.
These clusters read data from and write data to Amazon Simple storage service (S3). EMR users can access their rented hadoop 
cluster via their browser,SSH or a web services API.

CHeck the below tutorial on EMR and access the url for further details-http://aws.amazon.com/elasticmapreduce/
http://s3.amazonaws.com/awsVideos/AmazonElasticMapReduce/ElasticMapReduce-PigTutorial.html 
-----------------------------------------------------------
Command line options:  many are disscussed later.
See full list by   pig -h
pig -e or -execute to execute single command  e.g pig -e fs -ls
pig -h or -help - list the available command line options
pig -h properties - list all the properties that pig will use if they are SET BY USER
pig -P or -propertyFile - (capital -P) -specify a property file that Pig should read.
pig -version - prints the version of pig.


Hadoop has no of Java properties it uses to determine its behavior.
In Pig 0.8 and later, these can be passed to PIG and then PIG will pass them on to Hadoop when it invokes Hadoop.
in earlier versions these properties had to be hadoop-site.xml so that hadoop client would pic them up.

properties can be passed on command line using -D. e.g Pig -D exectype=local.
you can also place these in conf/pig.properties file of your pig distribution
or finally use a separate properties file by using -P.
Command line takes precedence over properties file if you use both of them.
------------------------------
PG -18
Return Codes:  to communicate success of failure.
0-success, 1-retriable failure,2-failure,3-partial failure,4-illegal args passed to pig,5-IOexception(usually by UDF)
,6-PigException(a python UDF raised an exception), 7- parseException,8- throwable(an unexpected exception)
-----------------------------------





