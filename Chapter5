PG 33
Introduction To PIG LATIN:
Advanced features are covered in Chapter 6.
----------------------------
Basics: 
Pig latin is data flow language.
Each processing step results in new data set, or relation.
eg. input = load 'data' -  input is name of the relation resulting from loading of data set 'data'.
Relation name is called ALIAS.
Relation names are not variables. Once made, assignment is permanent. But you can reuse the names.

A = load 'NYSE_dividends' (exchange, symbol, date, dividends);

Field Names - dividends,symbol.
A - relation name(alias).

The fields contain different value for each record , but you cannot assign values to them.
Relation and filed names should start with alphabetic character. can contain alphanumeric and _(underscore).

Case Sensitivity:
-------------------
UDF,Relation, field names are case sensitive. e.g COUNT is not the same as count UDF.
Keyworkds are not case sensive . e.g LOAD is equivalent to load.

Comments -SQL style --,/* */
---------------------------------
Input and Output:

LOAD - 
------------
First step to any data flow is to specify your input. Pig latin we specify the input with the 'LOAD' statement.
Defaults are - load looks data on HDFS , tab delimited, default load function is PigStorage.
[Here it is FUNCTION,so u can pass arguments]
Default incase of relative paths are home directory on HDFS, /users/yourlogin . 
you can specify URLs e.g. hdfs://nn.acme.com/data/examples/NYSE_dividends, here the data is red from HDFS in
nn.acme.com as a Namenode.

Usually most data wont be tab delimited, and you may also load data from storages other than HDFS. 
You specify the function for loading your data with the 'USING' clause.
e.g divs=load 'NYSE_dividends' using HBaseStorage();  using the loader for Hbase.
You can pas arguments to your load function via USING clause.
e.g divs=load 'NYSE_dividends' using PigStorage(',') - for comma separated text data.

Load stmt also can have 'AS' clause, allows you to specify the schema of the data you are loading.
divs=load 'NYSE_dividends' as (exchange,symbol,date,dividends);
You can specify file or directory as input. If directory then pig will read all the files in directory, 
also it reads sub directories in the input directory.

PG-35:
PigStorage and TextLoader are two built-in Pig load functions that operates on HDFS files, supports globs.
GLOBS, you can read multiple files under different directories and few files under a single directory.
Glob in Hadoop 0.20 -Table 5.1:
? , *, [abc] matches single char from char set a,b,c, [a-z] matches single char from char range(a..z) inclusive.
[^abc] - single char not matching a,b,c. ^ must occur immediately to the right of the opening bracket.
[^a-z], \c removes any special meaning of character c. 
{ab,cd} - matching string from string set{ab,cd}
--------------------
STORE :
  once you finished processing your data, you will want to write it out somewhere. STORE statement stores your data on HDFS
  in TAB delimited file using PigStorage function. you can specify path or URL. Default store function is PigStorage.
  store processed into '/data/examples/processed';
By USING clause, you can use a different store function.
 store processed into 'processed' using HbaseStorage();
 You can pass arguments to your store function. e.g 
 store processed into 'processed' using PigStorage(',');
 Here processed will be a directory, not a single file. 
 the no of part files created depends on the parallelism of the last job before the STORE.
 For maponly job, o/p depends on no of maps , controlled by Hadoop not PIG.
 if it has reduces, the o/p part files determined by the paralel level set for that job.
 ----------------------
DUMP:
 If we want ot see the output on screen instead of storing it,useful for debugging and prototyping sesssions.
 DUMP directs the output of your script to your screen.
 dump processed;
 
 maps are surrouned by [],bags {}, tuples (). missings values are rep by nulls and fields are separated by commas.
 ------------------------------------
 
Relational Operators  PG 37:
  These relational operators are the MAIN TOOLS PigLatin provides to operate on your data.
  They allows you to transform data by SORTING,GROUPING,JOINING,PROJECTION and filtering.
  Basics are covered here. advanced in next chapters.
  ----
FOREACH - 
  foreach - takes a SET OF EXPRESSONS and applies to EVERY RECORD in the data pipeline, hence the name foreach.
  









