PG - 1
Pig provides an engine for executing data flows in parallel on Hadoop.
These data flows are expressed in a language called - pig Latin.
Pig is an Apache open source project: This means users are 
free to download it as source or binary, 
use it for themselves, 
contribute to it, and—
under the terms of the Apache License: 
—use it in their products and change it as they see fit.
---------------------------------------
Pig On Hadoop:
Pig runs on Hadoop, means it makes use of both HDFS and MR.
HDFS:
it reads data from HDFS, stores intermediate data in HDFS and writes final results on HDFS.
It can also read input from and write output to sources other than HDFS.
MR:
In the map phase, the application has the opportunity to operate on each record in the input sepaately.
Many Maps are started at once so ,let input can be GBs or TBs, given enough machines, the map phase can usually be completed in under one minute.
Every record for a given key will go to the same reducer.
----------------------------------------------------
PG-4 :
Pig Latin, a Parallel Dataflow Language:
Pig Latin is a dataflow language.
Data flow means, users are allowed to describe how data from one or more inputs should be read, processed and then stored to one or more outputs in parallel.
data flows can be simple - simple linear flows like word count.
Complex workflows are - multiple inputs are joined, data is split into multiple streams that are processed by different operators.
Pig Latin is a DAG - directed acyclic graph - where the edges are data flows and the nodes are operators that process data.

For many procedural and OOP languages - main flow is the control flow, and data flow is a side effect of the program.
For Pig Latin, Data flow is the focus.

{we see how to integrate the data flow of pig latin script with the control flow, in chapter 9}
Comparing SQL and data flow languages:
SQL focus is to allow users to form queries.
It allows what questions they want answered, but now how they want it answered.
In Pig Latin - users describes exactly how to process input data.

SQL is oriented around answering one question. 
when user want sevaral data operations together, they must either write separate queres and  store the intermediate data into temp tables or 
write subqueries (confusing and difficult). 
Using subqueries creates - inside out design , the first step in the data pipeline is the innermost query.

Pig - is designed with a long series of data operations in mind. 
so no need to write the data pipeline in inside out subqueries and temp tables.

sql can operate only when data is in tables.
Pig does not require data to be loaded into tables first. it can operate on data as soon as it is copied into HDFS.
SQL is like universal language, means the barrier to adoption is very low.
But the goal is to make Pig latin as the native language of parallel data-processing systems such as hadoop. 
It may take some learning. but it allows users to utilize the power of hadoop much more fully.
-------------------------------------------
PG 5:
How Pig differs from MR?
MR provides the necessary parallel procesing , then why pig?
pig latin provides all of the standard data processing operations: like join,filter, group by, order by union etc. 
MR provides, group by operation directly(shuffle and reduce phase is group by) and order by operation inderectly similar to how it implements grouping. filter and projection can be implemented in the map phase. 
But other operators, like join are not provided and must be implemented by the user.
In addition pig provides some complex, non trivial implementations of the standard data operations - for e.g no of records per key in a data set is rarely evenly distributed and the data sent to the reducers is ofter skewed.
One reducer will get 10 or more times the data than other reducers.

Pig has join and order  by operators that will handle these skew and rebalances reducers in some cases.

but this rebalancing might took pig team months to write and rewriting the logic if it is in MR. i.e writing MR programs is time consuming.
in MR, the frame work has no opportunity to optimize or check users code.
PIG can analyze the pig latin , understand the data flow that the user is describing.
means it can do early error checking (like user is trying to add string to integer) and
optimizations( can these two grouping operations be combined?

Pig latin is much lower cost to write and maintain than java code for MR.
But MR has the flexibility to use users own data types and serialization framewors. but on the downside, system cannot check the users code for errors before and during runtime.

For less common algorithms or extreamly performance sensitive ones, MR is still right choice.
--------------------------------------------
PG 7

What is PIG is useful for?

Pig latin use cases tend to fall into 3 categories: 
1. Traditional ETL data pipleline
2. research on rawdata
3. Iterative processing.

Largest usecase is data pipeline.
Common esample is - web companies bring in logs from their web servers, cleansing the data and preprocessing before loading into data warehouse.
in this case data is loaded onto the grid, pig is used to clean records from BOTS and records with corrupt data.
It also used to join the web events data against user databases so that user cookies can be connected with known user information.
PG-8
Another ex of data pipeline is using pig offline to build behavior prediction models.
Pig is used to scan all user interactions with the webside and split the users into segments. then for each segment a mathematical model is produced..
these models predicts how members of that segment will respond to types of advertisements or news articles.
This way the website can show ads that are more likely to get clicked on or offer news that make the users to keep coming back to the site.

Traditionally for adhoc queries SQL is used, this make it easy to quickly form a question for the data to answer.
However for research on raw data, pig latin is preferred. this is because pig can operate even when there is no schema or schema is unknown, incomplete or inconsistent.
Pig can easily manange nested data- hence we can work on the data before it is cleansed and loaded into DW.
Python or perl scripting is used to process large datasets.. people who use these will often prefer the data flow paradigm of pig over the declarative query paradigm of SQL.
---------------
NOTE: PIG is like MR , is oriented around the batch processing of data.
if you need to process GB or TBs of data, pig is a good choice.
But it expects to read all the records of a file and write all of its output sequentially.
For writing single of small group of records, or to look up diff records in random order, PIG (like MR) is not a good choice..
You need to use any NoSQL databases for these use cases.
--------------------------------------------------------------------------------------

PIG PHILOSYPHY:
------------------------
To BEST contribute to Pig you should be aware of what is accepted and what is not.
The Pig team produced a Pig project's Philosophy- summariazed as below.
1) PIG eat anything: - Pig can operate on data whether it has metadata or not. 
































